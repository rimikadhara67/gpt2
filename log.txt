Trying to keep track of all the runtimes with different configurations and optimizations

This log/notes section is in the opposite chronological order for my convenience. So, the most recent logs appear frist. 
All of these were run on 1 A100 available in Colab Notebook

# ----- OPTIMIZATION #8 ----- #
implementing a weight decay - a regularization technique primarily used to prevent overfitting by penalizing large weights in a model
B = 8 ; T = 1024 ; dtype = bfloat16
-----------------------------
intuitiion is: keep simpler functions that generalize better to unseen data
we are also using fused adamW which is much faster
?? don't understand this fully but oh well.
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
num decayed parameter tensors: 50, with 124,354,560 parameters
num non-decayed parameter tensors: 98, with 121,344 parameters
using fused AdamW: True
step 50: loss = 10.913424 | lr = 0.000060 | norm = 24.9754 | time = 62.93439865112305 | throughput = 130.1672880901328
step 50: loss = 9.566368 | lr = 0.000120 | norm = 7.4813 | time = 60.66584587097168 | throughput = 135.03479399963058
step 50: loss = 9.142179 | lr = 0.000180 | norm = 5.8687 | time = 60.20522117614746 | throughput = 136.06793297930056
step 50: loss = 9.816030 | lr = 0.000240 | norm = 20.2128 | time = 53.946495056152344 | throughput = 151.85416571499283
step 50: loss = 8.867994 | lr = 0.000300 | norm = 2.4978 | time = 50.75263977050781 | throughput = 161.41032342440528
step 50: loss = 8.766240 | lr = 0.000360 | norm = 3.6531 | time = 50.29582977294922 | throughput = 162.87632666527617
step 50: loss = 8.539690 | lr = 0.000420 | norm = 3.0781 | time = 50.36115646362305 | throughput = 162.66504932064575
step 50: loss = 8.325491 | lr = 0.000480 | norm = 1.9814 | time = 50.689697265625 | throughput = 161.61075015051173
step 50: loss = 8.109070 | lr = 0.000540 | norm = 2.0749 | time = 50.7051944732666 | throughput = 161.56135648624885
step 50: loss = 7.841476 | lr = 0.000600 | norm = 2.4125 | time = 50.481319427490234 | throughput = 162.27785035941324
step 50: loss = 7.551817 | lr = 0.000600 | norm = 2.3555 | time = 50.41670799255371 | throughput = 162.48581722570853
step 50: loss = 7.305221 | lr = 0.000599 | norm = 1.7842 | time = 50.99201202392578 | throughput = 160.6526135143728
step 50: loss = 6.974848 | lr = 0.000597 | norm = 1.8551 | time = 51.15795135498047 | throughput = 160.1315100199467
step 50: loss = 6.713152 | lr = 0.000593 | norm = 1.8227 | time = 50.87161064147949 | throughput = 161.03284123896876
step 50: loss = 6.611022 | lr = 0.000587 | norm = 2.4085 | time = 51.32341384887695 | throughput = 159.6152591119824
step 50: loss = 6.403112 | lr = 0.000579 | norm = 1.7885 | time = 50.8575439453125 | throughput = 161.07738133813382
step 50: loss = 6.157389 | lr = 0.000571 | norm = 1.3865 | time = 51.00417137145996 | throughput = 160.61431407910175
step 50: loss = 7.003448 | lr = 0.000560 | norm = 27.9951 | time = 50.68039894104004 | throughput = 161.64040084866562
step 50: loss = 6.075782 | lr = 0.000548 | norm = 4.9848 | time = 50.739288330078125 | throughput = 161.4527966318322
step 50: loss = 5.838763 | lr = 0.000535 | norm = 1.8552 | time = 50.51064491271973 | throughput = 162.18363503684088
step 50: loss = 5.753511 | lr = 0.000521 | norm = 1.4796 | time = 50.539493560791016 | throughput = 162.09105835511232
step 50: loss = 5.755018 | lr = 0.000505 | norm = 3.2910 | time = 50.457000732421875 | throughput = 162.3560632040523
step 50: loss = 5.643024 | lr = 0.000489 | norm = 1.5931 | time = 50.33111572265625 | throughput = 162.76213794148856
step 50: loss = 5.642253 | lr = 0.000471 | norm = 2.9948 | time = 50.243377685546875 | throughput = 163.0463630703819
step 50: loss = 5.600631 | lr = 0.000453 | norm = 2.9028 | time = 50.5681037902832 | throughput = 161.99935109241954
step 50: loss = 5.499152 | lr = 0.000433 | norm = 1.2300 | time = 50.20284652709961 | throughput = 163.17799819534017
step 50: loss = 5.513709 | lr = 0.000413 | norm = 2.3374 | time = 50.24099349975586 | throughput = 163.0541004337386
step 50: loss = 5.488050 | lr = 0.000393 | norm = 2.2719 | time = 50.763845443725586 | throughput = 161.3746935125564
step 50: loss = 5.440851 | lr = 0.000372 | norm = 0.8616 | time = 50.888776779174805 | throughput = 160.9785205792647
step 50: loss = 5.421604 | lr = 0.000351 | norm = 1.6157 | time = 50.65298080444336 | throughput = 161.72789577037852
step 50: loss = 5.376632 | lr = 0.000330 | norm = 1.2333 | time = 50.71854591369629 | throughput = 161.5188261497022
step 50: loss = 5.354134 | lr = 0.000309 | norm = 0.6123 | time = 50.795793533325195 | throughput = 161.273196659986
step 50: loss = 5.340734 | lr = 0.000288 | norm = 1.0662 | time = 50.91238021850586 | throughput = 160.90388948309933
step 50: loss = 5.292768 | lr = 0.000267 | norm = 0.4190 | time = 50.76456069946289 | throughput = 161.37241979692095
step 50: loss = 5.277191 | lr = 0.000247 | norm = 1.0742 | time = 50.57883262634277 | throughput = 161.96498761684336
step 50: loss = 5.251352 | lr = 0.000227 | norm = 0.7546 | time = 50.38142204284668 | throughput = 162.5996184274661
step 50: loss = 5.232066 | lr = 0.000207 | norm = 0.5649 | time = 50.48227310180664 | throughput = 162.27478472451804
step 50: loss = 5.216723 | lr = 0.000189 | norm = 0.7401 | time = 50.17995834350586 | throughput = 163.25242727229534
step 50: loss = 5.192623 | lr = 0.000171 | norm = 0.4421 | time = 50.28867721557617 | throughput = 162.89949256137223
step 50: loss = 5.179278 | lr = 0.000155 | norm = 0.8503 | time = 50.56047439575195 | throughput = 162.0237962143861
step 50: loss = 5.166196 | lr = 0.000139 | norm = 0.8867 | time = 50.795555114746094 | throughput = 161.27395362634473
step 50: loss = 5.151527 | lr = 0.000125 | norm = 0.5175 | time = 50.2476692199707 | throughput = 163.0324376666635
step 50: loss = 5.141763 | lr = 0.000112 | norm = 0.4516 | time = 50.292015075683594 | throughput = 162.88868098985495
step 50: loss = 5.131406 | lr = 0.000100 | norm = 0.4951 | time = 50.75979232788086 | throughput = 161.3875791115161
step 50: loss = 5.120949 | lr = 0.000089 | norm = 0.3487 | time = 50.957679748535156 | throughput = 160.76085175827671
step 50: loss = 5.115551 | lr = 0.000081 | norm = 0.5040 | time = 50.72283744812012 | throughput = 161.50516043939515
step 50: loss = 5.109936 | lr = 0.000073 | norm = 0.6321 | time = 50.66657066345215 | throughput = 161.68451688618472
step 50: loss = 5.100560 | lr = 0.000067 | norm = 0.5463 | time = 50.66227912902832 | throughput = 161.6982129670154
step 50: loss = 5.092447 | lr = 0.000063 | norm = 0.4146 | time = 50.829410552978516 | throughput = 161.1665354934942
step 50: loss = 5.086170 | lr = 0.000061 | norm = 0.4089 | time = 50.624847412109375 | throughput = 161.81777168261624

--------------------------------------------------------------------------------

# ----- OPTIMIZATION #7 ----- #
cosine learning rate -- learning rate scheduler
B = 8 ; T = 1024 ; dtype = bfloat16
-----------------------------
defining our learning rate scheduler -- adjusts the lr between epochs or iterations
used to imporve training efficiency, convergence, and overall model performance instead of using a fixed lr
not time or throughput imporvement but loss improvement
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 50: loss = 10.986610 | lr = 0.000060 | norm = 26.1602 | time = 66.4072036743164 | throughput = 123.36011075208593
step 50: loss = 9.591007 | lr = 0.000120 | norm = 8.2883 | time = 65.29855728149414 | throughput = 125.45453285721588
step 50: loss = 9.116158 | lr = 0.000180 | norm = 4.5004 | time = 65.23442268371582 | throughput = 125.57787227946041
step 50: loss = 9.753958 | lr = 0.000240 | norm = 7.9801 | time = 59.917449951171875 | throughput = 136.72143935824792
step 50: loss = 9.097754 | lr = 0.000300 | norm = 4.2193 | time = 56.08797073364258 | throughput = 146.05627361530287
step 50: loss = 8.715553 | lr = 0.000360 | norm = 3.3339 | time = 56.366682052612305 | throughput = 145.33408215075775
step 50: loss = 8.586468 | lr = 0.000420 | norm = 3.3039 | time = 56.113243103027344 | throughput = 145.99049256445554
step 50: loss = 8.370012 | lr = 0.000480 | norm = 2.7065 | time = 55.52482604980469 | throughput = 147.5376076397238
step 50: loss = 8.102808 | lr = 0.000540 | norm = 2.4430 | time = 55.563926696777344 | throughput = 147.4337845974289
step 50: loss = 7.882781 | lr = 0.000600 | norm = 3.0185 | time = 55.845022201538086 | throughput = 146.6916777369349
step 50: loss = 7.625112 | lr = 0.000600 | norm = 3.0457 | time = 55.434465408325195 | throughput = 147.7781004950346
step 50: loss = 7.347375 | lr = 0.000599 | norm = 2.7242 | time = 55.518388748168945 | throughput = 147.55471447773564
step 50: loss = 7.085917 | lr = 0.000597 | norm = 2.0312 | time = 55.35721778869629 | throughput = 147.9843158171286
step 50: loss = 6.866299 | lr = 0.000593 | norm = 1.5215 | time = 55.612802505493164 | throughput = 147.3042110976305
step 50: loss = 6.669054 | lr = 0.000587 | norm = 1.4015 | time = 55.42612075805664 | throughput = 147.800349148722
step 50: loss = 6.478869 | lr = 0.000579 | norm = 1.3508 | time = 55.596351623535156 | throughput = 147.34779820573956
step 50: loss = 6.322378 | lr = 0.000571 | norm = 1.2785 | time = 55.5875301361084 | throughput = 147.371181629073
step 50: loss = 6.184790 | lr = 0.000560 | norm = 1.3641 | time = 55.89890480041504 | throughput = 146.5502773131107
step 50: loss = 6.062169 | lr = 0.000548 | norm = 1.0579 | time = 55.60588836669922 | throughput = 147.32252717512478
step 50: loss = 5.963470 | lr = 0.000535 | norm = 1.0494 | time = 55.61184883117676 | throughput = 147.30673718237279
step 50: loss = 5.871449 | lr = 0.000521 | norm = 1.2289 | time = 55.59206008911133 | throughput = 147.35917299824163
step 50: loss = 5.784989 | lr = 0.000505 | norm = 1.1597 | time = 55.68385124206543 | throughput = 147.11626112907024
step 50: loss = 5.735068 | lr = 0.000489 | norm = 1.6814 | time = 55.294036865234375 | throughput = 148.15340793377027
step 50: loss = 5.659874 | lr = 0.000471 | norm = 0.9647 | time = 55.243730545043945 | throughput = 148.2883201256749
step 50: loss = 5.650219 | lr = 0.000453 | norm = 1.8686 | time = 55.169105529785156 | throughput = 148.4889037321302
step 50: loss = 5.787306 | lr = 0.000433 | norm = 13.8366 | time = 55.46855926513672 | throughput = 147.68726840087342
step 50: loss = 5.645450 | lr = 0.000413 | norm = 2.7561 | time = 55.18317222595215 | throughput = 148.45105255017174
step 50: loss = 5.632334 | lr = 0.000393 | norm = 2.7056 | time = 55.435895919799805 | throughput = 147.7742871126594
step 50: loss = 5.544000 | lr = 0.000372 | norm = 1.4241 | time = 55.196523666381836 | throughput = 148.41514385061618
step 50: loss = 5.553920 | lr = 0.000351 | norm = 2.2181 | time = 55.744171142578125 | throughput = 146.95706891124343
step 50: loss = 5.608188 | lr = 0.000330 | norm = 3.8440 | time = 55.62162399291992 | throughput = 147.28084892024657
step 50: loss = 5.579007 | lr = 0.000309 | norm = 3.0597 | time = 55.61089515686035 | throughput = 147.309263353755
step 50: loss = 5.524154 | lr = 0.000288 | norm = 1.0725 | time = 55.98711967468262 | throughput = 146.3193685904943
step 50: loss = 5.505802 | lr = 0.000267 | norm = 1.1271 | time = 56.15091323852539 | throughput = 145.89255147464695
step 50: loss = 5.503031 | lr = 0.000247 | norm = 1.6348 | time = 55.73844909667969 | throughput = 146.97215535708176
step 50: loss = 5.488425 | lr = 0.000227 | norm = 1.6654 | time = 55.86099624633789 | throughput = 146.64972969466234
step 50: loss = 5.466415 | lr = 0.000207 | norm = 1.3353 | time = 55.40037155151367 | throughput = 147.86904438687245
step 50: loss = 5.441276 | lr = 0.000189 | norm = 0.6850 | time = 55.49764633178711 | throughput = 147.6098635070927
step 50: loss = 5.423060 | lr = 0.000171 | norm = 0.4885 | time = 55.24325370788574 | throughput = 148.28960008976856
step 50: loss = 5.416129 | lr = 0.000155 | norm = 0.8640 | time = 55.18770217895508 | throughput = 148.43886729395095
step 50: loss = 5.404191 | lr = 0.000139 | norm = 0.7946 | time = 55.188894271850586 | throughput = 148.4356609800457
step 50: loss = 5.393231 | lr = 0.000125 | norm = 0.4838 | time = 55.38654327392578 | throughput = 147.90596263581108
step 50: loss = 5.383036 | lr = 0.000112 | norm = 0.4433 | time = 55.182456970214844 | throughput = 148.45297672087517
step 50: loss = 5.373357 | lr = 0.000100 | norm = 0.5984 | time = 55.15623092651367 | throughput = 148.52356410854924
step 50: loss = 5.365525 | lr = 0.000089 | norm = 0.6447 | time = 56.0450553894043 | throughput = 146.1681131918152
step 50: loss = 5.358490 | lr = 0.000081 | norm = 0.5611 | time = 56.331634521484375 | throughput = 145.42450382609874
step 50: loss = 5.352262 | lr = 0.000073 | norm = 0.4447 | time = 55.87315559387207 | throughput = 146.61781517309655
step 50: loss = 5.350195 | lr = 0.000067 | norm = 0.3642 | time = 55.87935447692871 | throughput = 146.60155037013334
step 50: loss = 5.346507 | lr = 0.000063 | norm = 0.3351 | time = 55.78494071960449 | throughput = 146.849667568457
step 50: loss = 5.345114 | lr = 0.000061 | norm = 0.3337 | time = 56.04124069213867
--------------------------------------------------------------------------------

# ----- OPTIMIZATION #6 ----- #
tweaking hyperparams of the optimizer and clipping the gradients
B = 8 ; T = 1024 ; dtype = bfloat16
-----------------------------
once gradients are calculated, then we clip them to have a mx and global norm
- take every single gradient, sqare it and add and sqrt.
- in order to reduce the really high losses that usually shcoks your models
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 10.926804 | norm = 23.5944 | time = 66.79821014404297 | throughput = 122.63801653270134
step 2: loss = 9.537062 | norm = 4.0231 | time = 56.36000633239746 | throughput = 145.35129665681012
step 3: loss = 8.820678 | norm = 2.4451 | time = 56.0145378112793 | throughput = 146.24774781861055
step 4: loss = 8.682210 | norm = 3.9086 | time = 56.14042282104492 | throughput = 145.91981300377967
step 5: loss = 8.001700 | norm = 3.2836 | time = 55.77874183654785 | throughput = 146.865987476117
step 6: loss = 7.541465 | norm = 2.0643 | time = 55.86957931518555 | throughput = 146.62720035504879
step 7: loss = 7.212475 | norm = 2.7018 | time = 55.53913116455078 | throughput = 147.49960664182564
step 8: loss = 6.887771 | norm = 2.0886 | time = 55.28545379638672 | throughput = 148.17640875610218
step 9: loss = 6.608338 | norm = 1.7679 | time = 55.67741394042969 | throughput = 147.13327039155905
step 10: loss = 6.403276 | norm = 2.2939 | time = 56.04839324951172 | throughput = 146.15940841571523
step 11: loss = 6.247435 | norm = 2.5505 | time = 55.7098388671875 | throughput = 147.04763407285674
step 12: loss = 6.131461 | norm = 2.5725 | time = 55.815935134887695 | throughput = 146.76812240452097
step 13: loss = 6.048098 | norm = 2.4968 | time = 55.64284324645996 | throughput = 147.22468375160145
step 14: loss = 5.999734 | norm = 2.4208 | time = 55.8476448059082 | throughput = 146.68478909845373
step 15: loss = 5.970263 | norm = 2.3311 | time = 55.829524993896484 | throughput = 146.73239653920723
step 16: loss = 5.952134 | norm = 2.1811 | time = 55.26399612426758 | throughput = 148.23394206925116
step 17: loss = 5.933623 | norm = 1.9388 | time = 55.18651008605957 | throughput = 148.4420737463764
step 18: loss = 5.912395 | norm = 1.5688 | time = 55.43208122253418 | throughput = 147.78445656970567
step 19: loss = 5.888231 | norm = 1.0763 | time = 55.205345153808594 | throughput = 148.39142798901307
step 20: loss = 5.868648 | norm = 0.5815 | time = 55.16862869262695 | throughput = 148.490187161292
step 21: loss = 5.865623 | norm = 0.4711 | time = 55.24778366088867 | throughput = 148.27744132294174
step 22: loss = 5.869486 | norm = 0.5169 | time = 55.42159080505371 | throughput = 147.81242979501408
step 23: loss = 5.873492 | norm = 0.4982 | time = 55.390119552612305 | throughput = 147.89641304563042
step 24: loss = 5.875377 | norm = 0.6133 | time = 55.71866035461426 | throughput = 147.02435320345228
step 25: loss = 5.874604 | norm = 0.7754 | time = 55.85289001464844 | throughput = 146.67101376225114
step 26: loss = 5.869070 | norm = 0.8597 | time = 56.0457706451416 | throughput = 146.1662477953657
step 27: loss = 5.860271 | norm = 0.8489 | time = 55.941104888916016 | throughput = 146.43972471167862
step 28: loss = 5.851637 | norm = 0.7483 | time = 55.71556091308594 | throughput = 147.03253212830782
step 29: loss = 5.844303 | norm = 0.5910 | time = 55.968284606933594 | throughput = 146.36860960689762
step 30: loss = 5.840436 | norm = 0.4187 | time = 56.19072914123535 | throughput = 145.78917421429813
step 31: loss = 5.839907 | norm = 0.3296 | time = 55.71866035461426 | throughput = 147.02435320345228
step 32: loss = 5.842033 | norm = 0.3347 | time = 55.750370025634766 | throughput = 146.9407287562972
step 33: loss = 5.842043 | norm = 0.3316 | time = 55.52029609680176 | throughput = 147.549645371432
step 34: loss = 5.841835 | norm = 0.3196 | time = 55.505990982055664 | throughput = 147.58767216044052
step 35: loss = 5.841545 | norm = 0.3758 | time = 55.2983283996582 | throughput = 148.1419102001397
step 36: loss = 5.842877 | norm = 0.4778 | time = 55.527687072753906 | throughput = 147.53000587376556
step 37: loss = 5.844925 | norm = 0.5597 | time = 55.483102798461914 | throughput = 147.64855580908673
step 38: loss = 5.846768 | norm = 0.5769 | time = 56.03456497192383 | throughput = 146.1954778109656
step 39: loss = 5.845701 | norm = 0.5276 | time = 55.86099624633789 | throughput = 146.64972969466234
step 40: loss = 5.846416 | norm = 0.4480 | time = 55.62472343444824 | throughput = 147.27264234677915
step 41: loss = 5.842598 | norm = 0.3777 | time = 55.663347244262695 | throughput = 147.17045247120603
step 42: loss = 5.841492 | norm = 0.3398 | time = 55.85026741027832 | throughput = 146.67790110692286
step 43: loss = 5.839560 | norm = 0.3264 | time = 55.762529373168945 | throughput = 146.9086874660624
step 44: loss = 5.839067 | norm = 0.3465 | time = 55.51648139953613 | throughput = 147.55978393235216
step 45: loss = 5.837989 | norm = 0.4057 | time = 55.23967742919922 | throughput = 148.2992005248347
step 46: loss = 5.837263 | norm = 0.4664 | time = 55.61423301696777 | throughput = 147.3004221329572
step 47: loss = 5.837836 | norm = 0.4944 | time = 55.37271499633789 | throughput = 147.94289932400432
step 48: loss = 5.839626 | norm = 0.4888 | time = 55.454254150390625 | throughput = 147.7253661690858
step 49: loss = 5.838666 | norm = 0.4586 | time = 55.62162399291992 | throughput = 147.28084892024657
step 50: loss = 5.839334 | norm = 0.4231 | time = 55.6788444519043 | throughput = 147.12949021555747

--------------------------------------------------------------------------------

# ----- OPTIMIZATION #5 ----- #
Look for ugly numbers
B = 8 ; T = 1024 ; dtype = bfloat16
-----------------------------
--> fix 50257 because it can't be divided by 2 multiple times
    we change the vocab size to 50304 by doing: model = GPT(GPTConfig(vocab_size=50304))
    we are essentially adding fake tokens which increases are FLOPs technically but makes it more efficient to run on our GPUs.
--> time is faster a little but, but throughput is decently better
--> although we are adding rows into our data that we are never going to parse into, it still runs faster because of block tiles.
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 10.881217956542969 | time = 65.29402732849121 | throughput = 125.46323661100624
step 2: loss = 9.530352592468262 | time = 63.91096115112305 | throughput = 128.17832579030224
step 3: loss = 9.616275787353516 | time = 61.194419860839844 | throughput = 133.86841510433712
step 4: loss = 8.520885467529297 | time = 54.21257019042969 | throughput = 151.10886591844633
step 5: loss = 7.993741035461426 | time = 54.14462089538574 | throughput = 151.2985013936653
step 6: loss = 7.583930969238281 | time = 54.364919662475586 | throughput = 150.6854061563965
step 7: loss = 7.250425815582275 | time = 54.36301231384277 | throughput = 150.69069301581035
step 8: loss = 6.94461727142334 | time = 54.5506477355957 | throughput = 150.1723689827886
step 9: loss = 6.66085147857666 | time = 54.61716651916504 | throughput = 149.98947257956792
step 10: loss = 6.42510986328125 | time = 55.110931396484375 | throughput = 148.64564601647402
step 11: loss = 6.244624137878418 | time = 54.8856258392334 | throughput = 149.25583656448327
step 12: loss = 6.115160942077637 | time = 54.80670928955078 | throughput = 149.4707510483913
step 13: loss = 6.028721809387207 | time = 54.64529991149902 | throughput = 149.91225253164282
step 14: loss = 5.9799089431762695 | time = 55.33552169799805 | throughput = 148.0423378803416
step 15: loss = 5.958698272705078 | time = 54.250478744506836 | throughput = 151.00327572370938
step 16: loss = 5.951395034790039 | time = 54.152488708496094 | throughput = 151.27651923991334
step 17: loss = 5.946996212005615 | time = 54.228782653808594 | throughput = 151.06368978070097
step 18: loss = 5.935845375061035 | time = 54.29244041442871 | throughput = 150.8865679543648
step 19: loss = 5.915721893310547 | time = 54.19445037841797 | throughput = 151.1593888820455
step 20: loss = 5.891995906829834 | time = 54.29530143737793 | throughput = 150.87861717552727
step 21: loss = 5.8673858642578125 | time = 54.69012260437012 | throughput = 149.78938809958717
step 22: loss = 5.857439041137695 | time = 54.58354949951172 | throughput = 150.08184837948806
step 23: loss = 5.857667922973633 | time = 54.54540252685547 | throughput = 150.18680989596993
step 24: loss = 5.863768577575684 | time = 54.55636978149414 | throughput = 150.1566184262278
step 25: loss = 5.87266731262207 | time = 54.56876754760742 | throughput = 150.12250355211074
step 26: loss = 5.882365703582764 | time = 54.622650146484375 | throughput = 149.97441497311266
step 27: loss = 5.885629653930664 | time = 54.67057228088379 | throughput = 149.84295313229106
step 28: loss = 5.878398418426514 | time = 55.227041244506836 | throughput = 148.33313202008299
step 29: loss = 5.8651933670043945 | time = 54.68153953552246 | throughput = 149.81289973882826
step 30: loss = 5.855475425720215 | time = 54.605960845947266 | throughput = 150.02025187526743
step 31: loss = 5.850263595581055 | time = 54.36372756958008 | throughput = 150.68871040005612
step 32: loss = 5.84668493270874 | time = 54.84652519226074 | throughput = 149.36224257204088
step 33: loss = 5.845444679260254 | time = 54.34060096740723 | throughput = 150.75284141434972
step 34: loss = 5.842655658721924 | time = 54.657936096191406 | throughput = 149.87759482141922
step 35: loss = 5.839068412780762 | time = 54.43572998046875 | throughput = 150.48939369306237
step 36: loss = 5.838993072509766 | time = 54.28886413574219 | throughput = 150.89650760636616
step 37: loss = 5.840705871582031 | time = 54.60405349731445 | throughput = 150.02549216246192
step 38: loss = 5.843314170837402 | time = 55.236101150512695 | throughput = 148.3088022030672
step 39: loss = 5.848499298095703 | time = 54.70108985900879 | throughput = 149.7593561867735
step 40: loss = 5.852920055389404 | time = 54.81696128845215 | throughput = 149.4427966718714
step 41: loss = 5.855532646179199 | time = 54.61835861206055 | throughput = 149.9861989296596
step 42: loss = 5.856098651885986 | time = 54.80551719665527 | throughput = 149.47400223603674
step 43: loss = 5.853320121765137 | time = 54.474830627441406 | throughput = 150.38137623641043
step 44: loss = 5.850739479064941 | time = 54.32009696960449 | throughput = 150.8097455088112
step 45: loss = 5.846670150756836 | time = 54.18896675109863 | throughput = 151.17468538618914
step 46: loss = 5.8450775146484375 | time = 54.2905330657959 | throughput = 150.89186893913777
step 47: loss = 5.844275951385498 | time = 54.17370796203613 | throughput = 151.21726586891177
step 48: loss = 5.846859455108643 | time = 54.482221603393555 | throughput = 150.36097572588233
step 49: loss = 5.845147132873535 | time = 54.24070358276367 | throughput = 151.03048926163285
step 50: loss = 5.847914695739746 | time = 54.46457862854004 | throughput = 150.40968288529643

--------------------------------------------------------------------------------

# ----- OPTIMIZATION #4 ----- #
Flash-Attention
B = 8 ; T = 1024 ; dtype = bfloat16
-----------------------------
--> Kernel-Fusion algo that torch.compile cannot find -- the reason it cannot find is because it requires algorithmic rewrite of attention itself
    Flash Attention does more FLOPs but is faster because it's mindful of its memory hierarchy -- fewer reads and writes: don't materialize the matrix into HBM
    The "att" matrix which is the core of attention, is not ever read/written onto HBM during Flash-Attention -- it's a large matrix that contracts kvq
--> Online softmax trick which helps you get the softmax without ever knowing all the inputs
--> F.scaled_dot_product_attention(q, k, v, is_causal=True)
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 10.937248229980469 | time = 68.30883026123047 | throughput = 119.92593005430913
step 2: loss = 9.572845458984375 | time = 66.1923885345459 | throughput = 123.76045314824353
step 3: loss = 8.843436241149902 | time = 62.46638298034668 | throughput = 131.14253794040528
step 4: loss = 8.382768630981445 | time = 56.54478073120117 | throughput = 144.8763244647209
step 5: loss = 7.959447860717773 | time = 56.46800994873047 | throughput = 145.0732903007887
step 6: loss = 7.530279159545898 | time = 56.37979507446289 | throughput = 145.3002798108883
step 7: loss = 7.1631975173950195 | time = 55.98855018615723 | throughput = 146.31563012012793
step 8: loss = 6.8516645431518555 | time = 56.10203742980957 | throughput = 146.0196523209907
step 9: loss = 6.587648391723633 | time = 55.99474906921387 | throughput = 146.29943228916073
step 10: loss = 6.37211799621582 | time = 56.764841079711914 | throughput = 144.31468219027337
step 11: loss = 6.199061393737793 | time = 56.34808540344238 | throughput = 145.3820469914234
step 12: loss = 6.065949440002441 | time = 56.122779846191406 | throughput = 145.96568492242858
step 13: loss = 5.974018096923828 | time = 56.0002326965332 | throughput = 146.2851064279085
step 14: loss = 5.921084403991699 | time = 56.55980110168457 | throughput = 144.8378502122422
step 15: loss = 5.8961076736450195 | time = 56.3960075378418 | throughput = 145.2585095585562
step 16: loss = 5.886222839355469 | time = 56.199073791503906 | throughput = 145.7675268882893
step 17: loss = 5.881736755371094 | time = 56.234121322631836 | throughput = 145.6766782751004
step 18: loss = 5.87634801864624 | time = 56.46920204162598 | throughput = 145.0702277315927
step 19: loss = 5.87099027633667 | time = 56.13303184509277 | throughput = 145.93902610867357
step 20: loss = 5.865942478179932 | time = 56.14280700683594 | throughput = 145.91361630711737
step 21: loss = 5.861298561096191 | time = 55.81402778625488 | throughput = 146.77313795327657
step 22: loss = 5.858740329742432 | time = 56.099653244018555 | throughput = 146.02585802744593
step 23: loss = 5.85703182220459 | time = 56.17046356201172 | throughput = 145.84177306915228
step 24: loss = 5.856447696685791 | time = 55.788516998291016 | throughput = 146.8402538868518
step 25: loss = 5.858950138092041 | time = 55.816650390625 | throughput = 146.76624166211045
step 26: loss = 5.863671779632568 | time = 56.120872497558594 | throughput = 145.97064577633523
step 27: loss = 5.866368770599365 | time = 55.9542179107666 | throughput = 146.4054061673108
step 28: loss = 5.859160423278809 | time = 56.43868446350098 | throughput = 145.14867024049408
step 29: loss = 5.850503921508789 | time = 56.68163299560547 | throughput = 144.52653473542526
step 30: loss = 5.843613624572754 | time = 56.89382553100586 | throughput = 143.9875052089008
step 31: loss = 5.841170310974121 | time = 56.391239166259766 | throughput = 145.27079243368482
step 32: loss = 5.837658882141113 | time = 56.25128746032715 | throughput = 145.63222229851442
step 33: loss = 5.836633205413818 | time = 56.40363693237305 | throughput = 145.2388612780779
step 34: loss = 5.836865425109863 | time = 56.81729316711426 | throughput = 144.18145503526932
step 35: loss = 5.838354110717773 | time = 56.047916412353516 | throughput = 146.16065189168035
step 36: loss = 5.839386463165283 | time = 56.00619316101074 | throughput = 146.26953802142975
step 37: loss = 5.842270851135254 | time = 55.94015121459961 | throughput = 146.44222123343135
step 38: loss = 5.844479560852051 | time = 56.070804595947266 | throughput = 146.10098890202315
step 39: loss = 5.846694469451904 | time = 55.79185485839844 | throughput = 146.83146887285903
step 40: loss = 5.848828315734863 | time = 55.710792541503906 | throughput = 147.04511686666552
step 41: loss = 5.848433017730713 | time = 55.75871467590332 | throughput = 146.918738131176
step 42: loss = 5.847661018371582 | time = 56.69760704040527 | throughput = 144.485815674055
step 43: loss = 5.847373962402344 | time = 56.287288665771484 | throughput = 145.53907630270325
step 44: loss = 5.84539794921875 | time = 56.3197135925293 | throughput = 145.45528514702272
step 45: loss = 5.844130039215088 | time = 56.19692802429199 | throughput = 145.77309272953286
step 46: loss = 5.844479560852051 | time = 56.6709041595459 | throughput = 144.55389624518816
step 47: loss = 5.844901084899902 | time = 56.39839172363281 | throughput = 145.25236889986135
step 48: loss = 5.844442367553711 | time = 56.113481521606445 | throughput = 145.9898722706357
step 49: loss = 5.845497131347656 | time = 56.15687370300293 | throughput = 145.8770665070328
step 50: loss = 5.846281051635742 | time = 56.1978816986084 | throughput = 145.77061896984017
--------------------------------------------------------------------------------

# ----- OPTIMIZATION #3 ----- #
Using torch.compile : gcc of neural networks
B = 8 ; T = 1024 ; dtype = bfloat16
-----------------------------
--> makes compilation a bit slow but the computations are a lot faster
    it aims to reduce pytorch overhead and reduce gpu reads and writes
--> insane speedup and throughput
--> analyze the entire thing to get a sense before hand of what you want to do 
    it has a bird's eye view of your entire code and knows exactly what computations you're going to need and when
    it optimizes that process to run in efficient code -- i think this is like jit
--> now that it knows which computations are when -- it makes the element-wise computations while the data is written on the gpu 
    this is also called kernel fusion -- it fuses a bunch of operations together to happen during one roundtrip that the data is making
    kind of checks like "oh i have this piece of data, are there any more computations that need to happen here?"
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 10.977867126464844 | time = 89.90073204040527 | throughput = 91.12272852592747
step 2: loss = 9.502176284790039 | time = 81.44664764404297 | throughput = 100.5811808952847
step 3: loss = 9.041337966918945 | time = 77.70490646362305 | throughput = 105.42448827005566
step 4: loss = 8.3361177444458 | time = 78.31120491027832 | throughput = 104.60827424869315
step 5: loss = 7.98040246963501 | time = 78.35197448730469 | throughput = 104.55384249860026
step 6: loss = 7.541772365570068 | time = 78.32121849060059 | throughput = 104.59489979695772
step 7: loss = 7.143893241882324 | time = 78.3078670501709 | throughput = 104.6127331593834
step 8: loss = 6.843379020690918 | time = 78.3846378326416 | throughput = 104.51027428985093
step 9: loss = 6.602263450622559 | time = 77.73804664611816 | throughput = 105.37954519608535
step 10: loss = 6.396775245666504 | time = 77.81124114990234 | throughput = 105.28041808532804
step 11: loss = 6.22437047958374 | time = 78.32670211791992 | throughput = 104.58757714153522
step 12: loss = 6.0895466804504395 | time = 77.86059379577637 | throughput = 105.21368513431995
step 13: loss = 5.995548248291016 | time = 77.87966728210449 | throughput = 105.18791728174719
step 14: loss = 5.940192699432373 | time = 78.61208915710449 | throughput = 104.20789076891815
step 15: loss = 5.912414073944092 | time = 78.36794853210449 | throughput = 104.53253088083626
step 16: loss = 5.8999223709106445 | time = 78.43232154846191 | throughput = 104.44673622134609
step 17: loss = 5.894169807434082 | time = 78.6292552947998 | throughput = 104.18514036901712
step 18: loss = 5.888120174407959 | time = 78.48906517028809 | throughput = 104.37122651705462
step 19: loss = 5.880875587463379 | time = 77.80218124389648 | throughput = 105.29267777621152
step 20: loss = 5.873743057250977 | time = 78.02414894104004 | throughput = 104.99313496120786
step 21: loss = 5.866426944732666 | time = 77.64911651611328 | throughput = 105.50023448496088
step 22: loss = 5.864376068115234 | time = 77.62742042541504 | throughput = 105.52972074952471
step 23: loss = 5.8614501953125 | time = 77.81791687011719 | throughput = 105.27138645555038
step 24: loss = 5.860881805419922 | time = 78.43184471130371 | throughput = 104.44737121960561
step 25: loss = 5.860180854797363 | time = 78.35102081298828 | throughput = 104.5551151088769
step 26: loss = 5.864166259765625 | time = 78.54008674621582 | throughput = 104.30342439613746
step 27: loss = 5.871365070343018 | time = 78.34291458129883 | throughput = 104.56593354717371
step 28: loss = 5.866133689880371 | time = 78.30119132995605 | throughput = 104.62165212122319
step 29: loss = 5.857914924621582 | time = 77.9731273651123 | throughput = 105.06183703060454
step 30: loss = 5.849363327026367 | time = 77.82793045043945 | throughput = 105.25784191597688
step 31: loss = 5.843747138977051 | time = 77.81195640563965 | throughput = 105.27945033658428
step 32: loss = 5.839478492736816 | time = 77.88896560668945 | throughput = 105.17536002938566
step 33: loss = 5.836612701416016 | time = 77.88300514221191 | throughput = 105.183409205149
step 34: loss = 5.8363800048828125 | time = 78.05132865905762 | throughput = 104.95657333117472
step 35: loss = 5.837681293487549 | time = 78.42278480529785 | throughput = 104.45943765371858
step 36: loss = 5.840752601623535 | time = 78.3388614654541 | throughput = 104.57134360591277
step 37: loss = 5.84300422668457 | time = 78.27925682067871 | throughput = 104.650967992276
step 38: loss = 5.84693717956543 | time = 78.31835746765137 | throughput = 104.59872071989795
step 39: loss = 5.847560882568359 | time = 78.03678512573242 | throughput = 104.9761338425346
step 40: loss = 5.847979545593262 | time = 77.7578353881836 | throughput = 105.35272695161586
step 41: loss = 5.849864959716797 | time = 78.03702354431152 | throughput = 104.97581311963239
step 42: loss = 5.847596168518066 | time = 78.15337181091309 | throughput = 104.81953382408122
step 43: loss = 5.847255706787109 | time = 78.34672927856445 | throughput = 104.56084223851983
step 44: loss = 5.846600532531738 | time = 78.54509353637695 | throughput = 104.29677566309093
step 45: loss = 5.8443708419799805 | time = 78.3090591430664 | throughput = 104.6111406476441
step 46: loss = 5.844832420349121 | time = 78.56106758117676 | throughput = 104.27556870373799
step 47: loss = 5.84435510635376 | time = 78.60231399536133 | throughput = 104.22085029816611
step 48: loss = 5.845335960388184 | time = 78.31931114196777 | throughput = 104.59744704790027
step 49: loss = 5.8457417488098145 | time = 78.2926082611084 | throughput = 104.63312159277429
step 50: loss = 5.846532821655273 | time = 78.45473289489746 | throughput = 104.41690001002847
--------------------------------------------------------------------------------

# ----- OPTIMIZATION #2 ----- #
Precision change to BF16 to save memory -- using torch.autocast
B = 8 ; T = 1024 ; dtype = bfloat16
--> not a lot faster but oh well
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 10.999964714050293 | time = 214.4143581390381 | throughput = 38.20639658230283
step 2: loss = 9.49085521697998 | time = 205.1551342010498 | throughput = 39.93075792084213
step 3: loss = 8.866656303405762 | time = 203.86695861816406 | throughput = 40.18306868129298
step 4: loss = 8.420998573303223 | time = 203.7515640258789 | throughput = 40.20582634133555
step 5: loss = 8.000720977783203 | time = 203.76825332641602 | throughput = 40.20253334987001
step 6: loss = 7.5556440353393555 | time = 204.1482925415039 | throughput = 40.127692953075
step 7: loss = 7.171408176422119 | time = 203.4461498260498 | throughput = 40.266183493786
step 8: loss = 6.865958213806152 | time = 203.871488571167 | throughput = 40.18217582759423
step 9: loss = 6.613677978515625 | time = 203.82189750671387 | throughput = 40.19195238691248
step 10: loss = 6.396533966064453 | time = 203.81522178649902 | throughput = 40.193268825531106
step 11: loss = 6.2149128913879395 | time = 203.6752700805664 | throughput = 40.22088688901479
step 12: loss = 6.077444553375244 | time = 203.88555526733398 | throughput = 40.179403534785386
step 13: loss = 5.990556716918945 | time = 203.86195182800293 | throughput = 40.18405556575628
step 14: loss = 5.943273544311523 | time = 203.72271537780762 | throughput = 40.211519784663096
step 15: loss = 5.919780254364014 | time = 203.40657234191895 | throughput = 40.27401821721645
step 16: loss = 5.910135746002197 | time = 203.77635955810547 | throughput = 40.20093409149409
step 17: loss = 5.9018144607543945 | time = 203.78994941711426 | throughput = 40.198253267302874
step 18: loss = 5.891624450683594 | time = 203.77683639526367 | throughput = 40.200840021434374
step 19: loss = 5.880627155303955 | time = 203.3541202545166 | throughput = 40.284406284696615
step 20: loss = 5.870386123657227 | time = 203.89366149902344 | throughput = 40.17780611605347
step 21: loss = 5.865752696990967 | time = 203.66311073303223 | throughput = 40.22328820626884
step 22: loss = 5.861918926239014 | time = 203.7336826324463 | throughput = 40.209355145163194
step 23: loss = 5.859808921813965 | time = 203.70912551879883 | throughput = 40.214202378695205
step 24: loss = 5.8578925132751465 | time = 203.76014709472656 | throughput = 40.20413273549317
step 25: loss = 5.858131408691406 | time = 203.91392707824707 | throughput = 40.173813124870655
step 26: loss = 5.863741874694824 | time = 204.96249198913574 | throughput = 39.96828844388868
step 27: loss = 5.870166301727295 | time = 203.6440372467041 | throughput = 40.22705555614094
step 28: loss = 5.869153022766113 | time = 203.91321182250977 | throughput = 40.17395404045955
step 29: loss = 5.861891269683838 | time = 203.5973072052002 | throughput = 40.236288546505705
step 30: loss = 5.853564262390137 | time = 203.37319374084473 | throughput = 40.28062818563462
step 31: loss = 5.847261905670166 | time = 203.66287231445312 | throughput = 40.22333529378711
step 32: loss = 5.8431010246276855 | time = 203.7646770477295 | throughput = 40.20323894548769
step 33: loss = 5.839210033416748 | time = 203.627347946167 | throughput = 40.23035256622662
step 34: loss = 5.838056564331055 | time = 203.64141464233398 | throughput = 40.227573621937545
step 35: loss = 5.838972091674805 | time = 203.37390899658203 | throughput = 40.28048652070545
step 36: loss = 5.840801239013672 | time = 203.77635955810547 | throughput = 40.20093409149409
step 37: loss = 5.842396259307861 | time = 203.8893699645996 | throughput = 40.17865179250071
step 38: loss = 5.845513343811035 | time = 203.7339210510254 | throughput = 40.20930809037099
step 39: loss = 5.846497535705566 | time = 203.53341102600098 | throughput = 40.24892010950226
step 40: loss = 5.849394798278809 | time = 203.6113739013672 | throughput = 40.23350878211914
step 41: loss = 5.850845813751221 | time = 203.74131202697754 | throughput = 40.20784944643574
step 42: loss = 5.849950313568115 | time = 203.40847969055176 | throughput = 40.273640570258465
step 43: loss = 5.850064277648926 | time = 203.33504676818848 | throughput = 40.288185092554485
step 44: loss = 5.846957206726074 | time = 203.78518104553223 | throughput = 40.19919386665138
step 45: loss = 5.845311164855957 | time = 203.69362831115723 | throughput = 40.21726191398637
step 46: loss = 5.845368385314941 | time = 203.69577407836914 | throughput = 40.21683825825587
step 47: loss = 5.844884395599365 | time = 203.55510711669922 | throughput = 40.24463014481618
step 48: loss = 5.846238613128662 | time = 203.67670059204102 | throughput = 40.220604399952244
step 49: loss = 5.846622467041016 | time = 203.6271095275879 | throughput = 40.23039967028618
step 50: loss = 5.846555233001709 | time = 203.39035987854004 | throughput = 40.27722850233448

--------------------------------------------------------------------------------

# ----- OPTIMIZATION #1 ----- #
Precision change to TF32 to save memory 
B = 8 ; T = 1024 ; dtype = tensorfloat32
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 11.006056785583496 | time = 255.3417682647705 | throughput = 32.08249106940273
step 2: loss = 9.612740516662598 | time = 238.3575439453125 | throughput = 34.36853671339863
step 3: loss = 11.051552772521973 | time = 237.16211318969727 | throughput = 34.541773514420996
step 4: loss = 8.475252151489258 | time = 237.12611198425293 | throughput = 34.54701775122941
step 5: loss = 7.484814167022705 | time = 236.8175983428955 | throughput = 34.59202380786984
step 6: loss = 6.813332557678223 | time = 237.26272583007812 | throughput = 34.52712587423831
step 7: loss = 6.363587379455566 | time = 237.22124099731445 | throughput = 34.53316391719214
step 8: loss = 6.100831031799316 | time = 236.98759078979492 | throughput = 34.567210767023674
step 9: loss = 5.985622406005859 | time = 237.1804714202881 | throughput = 34.53909991385264
step 10: loss = 5.935225963592529 | time = 237.20741271972656 | throughput = 34.535177067502914
step 11: loss = 5.9066386222839355 | time = 237.0607852935791 | throughput = 34.5565378510618
step 12: loss = 5.899106502532959 | time = 236.9084358215332 | throughput = 34.578760235330584
step 13: loss = 5.909369945526123 | time = 237.1687889099121 | throughput = 34.54080124814276
step 14: loss = 5.923038005828857 | time = 237.02430725097656 | throughput = 34.56185610248735
step 15: loss = 5.920040130615234 | time = 236.7711067199707 | throughput = 34.5988161878581
step 16: loss = 5.896843433380127 | time = 237.12539672851562 | throughput = 34.54712195749747
step 17: loss = 5.863986968994141 | time = 237.1819019317627 | throughput = 34.53889159872257
step 18: loss = 5.831890106201172 | time = 236.88268661499023 | throughput = 34.58251895510881
step 19: loss = 5.820229530334473 | time = 237.1540069580078 | throughput = 34.54295419706121
step 20: loss = 5.827034950256348 | time = 237.17164993286133 | throughput = 34.540384579350004
step 21: loss = 5.837375164031982 | time = 237.09964752197266 | throughput = 34.55087380187196
step 22: loss = 5.84666109085083 | time = 236.87195777893066 | throughput = 34.58408532953268
step 23: loss = 5.84515380859375 | time = 237.1504306793213 | throughput = 34.54347511212137
step 24: loss = 5.835930347442627 | time = 237.0767593383789 | throughput = 34.55420945883432
step 25: loss = 5.825813293457031 | time = 236.95802688598633 | throughput = 34.5715235210902
step 26: loss = 5.818314075469971 | time = 237.10083961486816 | throughput = 34.550700087382964
step 27: loss = 5.814695835113525 | time = 237.18833923339844 | throughput = 34.53795421173254
step 28: loss = 5.812010765075684 | time = 236.83834075927734 | throughput = 34.588994221701434
step 29: loss = 5.812100410461426 | time = 237.23983764648438 | throughput = 34.530456947146696
step 30: loss = 5.810514450073242 | time = 237.17117309570312 | throughput = 34.54045402345069
step 31: loss = 5.807673454284668 | time = 237.1068000793457 | throughput = 34.54983154113935
step 32: loss = 5.805992126464844 | time = 237.08701133728027 | throughput = 34.55271528285474
step 33: loss = 5.802384853363037 | time = 237.59698867797852 | throughput = 34.47855145631847
step 34: loss = 5.798579216003418 | time = 237.14828491210938 | throughput = 34.543787668698826
step 35: loss = 5.794641494750977 | time = 237.2605800628662 | throughput = 34.527438135021804
step 36: loss = 5.789761066436768 | time = 237.24365234375 | throughput = 34.529901723694366
step 37: loss = 5.783792495727539 | time = 237.26749420166016 | throughput = 34.526431981607196
step 38: loss = 5.767065525054932 | time = 237.2124195098877 | throughput = 34.53444814114606
step 39: loss = 7.053713321685791 | time = 237.6255989074707 | throughput = 34.47440022314217
step 40: loss = 5.803648471832275 | time = 237.14685440063477 | throughput = 34.54399604289279
step 41: loss = 5.827505588531494 | time = 237.2722625732422 | throughput = 34.52573811686589
step 42: loss = 5.841487884521484 | time = 237.10942268371582 | throughput = 34.54944939462589
step 43: loss = 5.84422492980957 | time = 237.29228973388672 | throughput = 34.52282418947106
step 44: loss = 5.841358184814453 | time = 237.2746467590332 | throughput = 34.525391194953386
step 45: loss = 5.8339314460754395 | time = 237.29777336120605 | throughput = 34.52202641417303
step 46: loss = 5.825125217437744 | time = 237.54143714904785 | throughput = 34.48661462319875
step 47: loss = 5.816109657287598 | time = 237.0901107788086 | throughput = 34.5522635806715
step 48: loss = 5.810768127441406 | time = 237.17713356018066 | throughput = 34.539585992261706
step 49: loss = 5.809289932250977 | time = 236.87458038330078 | throughput = 34.58370242490368
step 50: loss = 5.809300899505615 | time = 237.19239234924316 | throughput = 34.537364031212526

--------------------------------------------------------------

# ----- BASELINE CHANGE ----- #
changing the baseline metric from time to throughput and batch to 8
B = 8 ; T = 1024 ; dtype = float32
-------------------------------
cuda
loaded 338025 tokens
1 Epoch = 41 batches
step 1: loss = 11.029096603393555 | time = 955.348014831543 | throughput = 8.574885667653268
step 2: loss = 9.574460983276367 | time = 577.4695873260498 | throughput = 14.186028458975187
step 3: loss = 8.52872085571289 | time = 577.6572227478027 | throughput = 14.181420533499528
step 4: loss = 7.646514892578125 | time = 577.7809619903564 | throughput = 14.178383399445982
step 5: loss = 7.036017417907715 | time = 577.7089595794678 | throughput = 14.180150513786753
step 6: loss = 6.524392127990723 | time = 577.6762962341309 | throughput = 14.180952296993334
step 7: loss = 6.202227592468262 | time = 577.7175426483154 | throughput = 14.179939841270954
step 8: loss = 6.019207000732422 | time = 577.674150466919 | throughput = 14.181004972056687
step 9: loss = 5.93194580078125 | time = 577.7349472045898 | throughput = 14.179512663441175
step 10: loss = 5.901697635650635 | time = 577.6910781860352 | throughput = 14.180589434967718
step 11: loss = 5.901927471160889 | time = 577.4860382080078 | throughput = 14.185624340668959
step 12: loss = 5.911057472229004 | time = 577.6858329772949 | throughput = 14.180718190335082
step 13: loss = 5.917102336883545 | time = 577.5485038757324 | throughput = 14.184090072134655
step 14: loss = 5.908847808837891 | time = 577.6937007904053 | throughput = 14.180525058160818
step 15: loss = 5.887024879455566 | time = 577.71897315979 | throughput = 14.179904729793584
step 16: loss = 5.86067533493042 | time = 577.6348114013672 | throughput = 14.181970750907224
step 17: loss = 5.839723587036133 | time = 577.6126384735107 | throughput = 14.182515156956152
step 18: loss = 5.82953405380249 | time = 577.6290893554688 | throughput = 14.182111238789608
step 19: loss = 5.8264312744140625 | time = 577.6517391204834 | throughput = 14.181555157217934
step 20: loss = 5.831329345703125 | time = 577.6374340057373 | throughput = 14.18190636155799
step 21: loss = 5.835185527801514 | time = 577.7146816253662 | throughput = 14.180010064747343
step 22: loss = 5.834731578826904 | time = 577.6903629302979 | throughput = 14.180606992380136
step 23: loss = 5.829263210296631 | time = 577.6104927062988 | throughput = 14.182567843630633
step 24: loss = 5.820180416107178 | time = 577.6731967926025 | throughput = 14.181028383321564
step 25: loss = 5.812677383422852 | time = 577.5022506713867 | throughput = 14.185226101675322
step 26: loss = 5.806911945343018 | time = 577.6028633117676 | throughput = 14.182755177199107
step 27: loss = 5.803328037261963 | time = 577.6698589324951 | throughput = 14.181110323357366
step 28: loss = 5.801115036010742 | time = 577.669620513916 | throughput = 14.181116176253301
step 29: loss = 5.7981367111206055 | time = 577.6259899139404 | throughput = 14.182187337554726
step 30: loss = 5.794368743896484 | time = 577.6870250701904 | throughput = 14.18068892754628
step 31: loss = 5.789984703063965 | time = 577.6476860046387 | throughput = 14.181654663348233
step 32: loss = 5.784980297088623 | time = 577.6004791259766 | throughput = 14.182813719954165
step 33: loss = 5.777536392211914 | time = 577.6736736297607 | throughput = 14.181016677679462
step 34: loss = 5.77039909362793 | time = 577.725887298584 | throughput = 14.17973502677085
step 35: loss = 5.763670921325684 | time = 577.7254104614258 | throughput = 14.179746730297182
step 36: loss = 5.755492687225342 | time = 577.6631832122803 | throughput = 14.181274206269773
step 37: loss = 5.744278430938721 | time = 577.7754783630371 | throughput = 14.17851796550748
step 38: loss = 5.728265762329102 | time = 577.8274536132812 | throughput = 14.177242615894476
step 39: loss = 5.693336486816406 | time = 577.6300430297852 | throughput = 14.182087823949255
step 40: loss = 5.645940780639648 | time = 577.587366104126 | throughput = 14.183135713746147
step 41: loss = 5.64841890335083 | time = 577.5902271270752 | throughput = 14.183065459308203
step 42: loss = 5.587576866149902 | time = 577.613353729248 | throughput = 14.182497594818313
step 43: loss = 5.5731658935546875 | time = 577.5179862976074 | throughput = 14.184839596975749
step 44: loss = 5.557539463043213 | time = 577.5907039642334 | throughput = 14.183053750302879
step 45: loss = 5.607537269592285 | time = 577.6212215423584 | throughput = 14.182304414172672
step 46: loss = 5.561897277832031 | time = 578.0611038208008 | throughput = 14.17151222570326
step 47: loss = 5.5241289138793945 | time = 577.5480270385742 | throughput = 14.184101782851142
step 48: loss = 5.522468566894531 | time = 577.6445865631104 | throughput = 14.181730757213607
step 49: loss = 5.507747650146484 | time = 577.556848526001 | throughput = 14.183885137726326
step 50: loss = 5.5220417976379395 | time = 577.6791572570801 | throughput = 14.180882064184251

--------------------------------------------------------------

# ----- BASELINE ----- #
B = 4 ; T = 1024 ; dtype = float32
------------------------
cuda
loaded 338025 tokens
1 Epoch = 82 batches
step 1: loss = 10.957832336425781 | time = 1589.1225337982178
step 2: loss = 9.744239807128906 | time = 1212.7771377563477
step 3: loss = 10.972354888916016 | time = 1230.9072017669678
step 4: loss = 8.774917602539062 | time = 1239.363670349121
step 5: loss = 7.651184558868408 | time = 1245.945692062378
step 6: loss = 6.87078857421875 | time = 1245.624303817749
step 7: loss = 6.329787731170654 | time = 1244.5142269134521
step 8: loss = 5.989801406860352 | time = 1257.246732711792
step 9: loss = 5.809932708740234 | time = 1256.6657066345215
step 10: loss = 5.739829063415527 | time = 1226.353645324707
step 11: loss = 5.726529121398926 | time = 1253.2141208648682
step 12: loss = 5.719888210296631 | time = 1255.2611827850342
step 13: loss = 5.71586799621582 | time = 1259.6900463104248
step 14: loss = 5.716298580169678 | time = 1260.7014179229736
step 15: loss = 5.709798336029053 | time = 1262.582778930664
step 16: loss = 5.691406726837158 | time = 1268.1748867034912
step 17: loss = 5.663188934326172 | time = 1268.4566974639893
step 18: loss = 5.636880397796631 | time = 1274.1649150848389
step 19: loss = 5.62070369720459 | time = 1275.9020328521729
step 20: loss = 5.6150031089782715 | time = 1272.2272872924805
step 21: loss = 5.619780540466309 | time = 1278.289794921875
step 22: loss = 5.62244987487793 | time = 1282.4633121490479
step 23: loss = 5.624851226806641 | time = 1289.5894050598145
step 24: loss = 5.618223190307617 | time = 1285.5684757232666
step 25: loss = 5.6100335121154785 | time = 1292.560338973999
step 26: loss = 5.600595474243164 | time = 1295.4905033111572
step 27: loss = 5.593338489532471 | time = 1298.9661693572998
step 28: loss = 5.587722301483154 | time = 1300.8184432983398
step 29: loss = 5.583390235900879 | time = 1293.1113243103027
step 30: loss = 5.575660705566406 | time = 1317.1510696411133
step 31: loss = 5.555104732513428 | time = 1307.7375888824463
step 32: loss = 5.494010925292969 | time = 1326.1082172393799
step 33: loss = 9.473379135131836 | time = 1327.1868228912354
step 34: loss = 5.924776077270508 | time = 1316.1354064941406
step 35: loss = 5.6139702796936035 | time = 1333.8656425476074
step 36: loss = 5.634500026702881 | time = 1317.8064823150635
step 37: loss = 5.63551139831543 | time = 1329.9860954284668
step 38: loss = 5.626481056213379 | time = 1320.6093311309814
step 39: loss = 5.609877109527588 | time = 1344.2010879516602
step 40: loss = 5.600749969482422 | time = 1335.1902961730957
step 41: loss = 5.597568511962891 | time = 1331.3891887664795
step 42: loss = 5.602120876312256 | time = 1336.2674713134766
step 43: loss = 5.612254619598389 | time = 1345.306634902954
step 44: loss = 5.614740371704102 | time = 1340.6474590301514
step 45: loss = 5.608066082000732 | time = 1354.4979095458984
step 46: loss = 5.599985599517822 | time = 1372.894048690796
step 47: loss = 5.594512939453125 | time = 1378.643274307251
step 48: loss = 5.5951409339904785 | time = 1382.4083805084229
step 49: loss = 5.598855972290039 | time = 1357.0613861083984
step 50: loss = 5.6001200675964355 | time = 1364.9451732635498

--------------------------------------------------------------

