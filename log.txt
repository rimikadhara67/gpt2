Trying to keep track of all the runtimes with different configurations and optimizations

This log/notes section is in the opposite chronological order for my convenience. So, the most recent logs appear frist. 

# ----- OPTIMization #1 ----- 
Precision change to TF32 to save memory 
B = 4 ; T = 1024 ; dtype = tensorfloat32
-----------------------------


# ----- BASELINE ----- #
B = 4 ; T = 1024 ; dtype = float32
------------------------
cuda
loaded 338025 tokens
1 Epoch = 82 batches
step 1: loss = 10.957832336425781 | time = 1589.1225337982178
step 2: loss = 9.744239807128906 | time = 1212.7771377563477
step 3: loss = 10.972354888916016 | time = 1230.9072017669678
step 4: loss = 8.774917602539062 | time = 1239.363670349121
step 5: loss = 7.651184558868408 | time = 1245.945692062378
step 6: loss = 6.87078857421875 | time = 1245.624303817749
step 7: loss = 6.329787731170654 | time = 1244.5142269134521
step 8: loss = 5.989801406860352 | time = 1257.246732711792
step 9: loss = 5.809932708740234 | time = 1256.6657066345215
step 10: loss = 5.739829063415527 | time = 1226.353645324707
step 11: loss = 5.726529121398926 | time = 1253.2141208648682
step 12: loss = 5.719888210296631 | time = 1255.2611827850342
step 13: loss = 5.71586799621582 | time = 1259.6900463104248
step 14: loss = 5.716298580169678 | time = 1260.7014179229736
step 15: loss = 5.709798336029053 | time = 1262.582778930664
step 16: loss = 5.691406726837158 | time = 1268.1748867034912
step 17: loss = 5.663188934326172 | time = 1268.4566974639893
step 18: loss = 5.636880397796631 | time = 1274.1649150848389
step 19: loss = 5.62070369720459 | time = 1275.9020328521729
step 20: loss = 5.6150031089782715 | time = 1272.2272872924805
step 21: loss = 5.619780540466309 | time = 1278.289794921875
step 22: loss = 5.62244987487793 | time = 1282.4633121490479
step 23: loss = 5.624851226806641 | time = 1289.5894050598145
step 24: loss = 5.618223190307617 | time = 1285.5684757232666
step 25: loss = 5.6100335121154785 | time = 1292.560338973999
step 26: loss = 5.600595474243164 | time = 1295.4905033111572
step 27: loss = 5.593338489532471 | time = 1298.9661693572998
step 28: loss = 5.587722301483154 | time = 1300.8184432983398
step 29: loss = 5.583390235900879 | time = 1293.1113243103027
step 30: loss = 5.575660705566406 | time = 1317.1510696411133
step 31: loss = 5.555104732513428 | time = 1307.7375888824463
step 32: loss = 5.494010925292969 | time = 1326.1082172393799
step 33: loss = 9.473379135131836 | time = 1327.1868228912354
step 34: loss = 5.924776077270508 | time = 1316.1354064941406
step 35: loss = 5.6139702796936035 | time = 1333.8656425476074
step 36: loss = 5.634500026702881 | time = 1317.8064823150635
step 37: loss = 5.63551139831543 | time = 1329.9860954284668
step 38: loss = 5.626481056213379 | time = 1320.6093311309814
step 39: loss = 5.609877109527588 | time = 1344.2010879516602
step 40: loss = 5.600749969482422 | time = 1335.1902961730957
step 41: loss = 5.597568511962891 | time = 1331.3891887664795
step 42: loss = 5.602120876312256 | time = 1336.2674713134766
step 43: loss = 5.612254619598389 | time = 1345.306634902954
step 44: loss = 5.614740371704102 | time = 1340.6474590301514
step 45: loss = 5.608066082000732 | time = 1354.4979095458984
step 46: loss = 5.599985599517822 | time = 1372.894048690796
step 47: loss = 5.594512939453125 | time = 1378.643274307251
step 48: loss = 5.5951409339904785 | time = 1382.4083805084229
step 49: loss = 5.598855972290039 | time = 1357.0613861083984
step 50: loss = 5.6001200675964355 | time = 1364.9451732635498

--------------------------------------------------------------

