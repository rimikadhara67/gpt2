Trying to keep track of all the runtimes with different configurations and optimizations

This log/notes section is in the opposite chronological order for my convenience. So, the most recent logs appear frist. 
All of these were run on 1 A100 available in Colab Notebook

# ----- OPTIMIZATION #1 ----- 
Precision change to TF32 to save memory 
B = 4 ; T = 1024 ; dtype = tensorfloat32
-----------------------------
cuda
loaded 338025 tokens
1 Epoch = 82 batches
step 1: loss = 11.01377010345459 | time = 139.5866870880127 | throughput = 29.343772572074695
step 2: loss = 9.444877624511719 | time = 126.04522705078125 | throughput = 32.496272138490404
step 3: loss = 10.960712432861328 | time = 125.89550018310547 | throughput = 32.534919786987444
step 4: loss = 8.35510540008545 | time = 125.89287757873535 | throughput = 32.535597555455816
step 5: loss = 7.325888633728027 | time = 125.9920597076416 | throughput = 32.509985228470484
step 6: loss = 6.6323442459106445 | time = 126.17254257202148 | throughput = 32.46348148736031
step 7: loss = 6.222462177276611 | time = 125.82206726074219 | throughput = 32.55390798429518
step 8: loss = 6.019113063812256 | time = 125.97370147705078 | throughput = 32.514722930056855
step 9: loss = 5.881724834442139 | time = 125.79631805419922 | throughput = 32.56057143290349
step 10: loss = 5.775393962860107 | time = 125.82683563232422 | throughput = 32.5526743116137
step 11: loss = 5.7256011962890625 | time = 125.72813034057617 | throughput = 32.57823041593501
step 12: loss = 5.719995021820068 | time = 125.77390670776367 | throughput = 32.56637332190911
step 13: loss = 5.729066848754883 | time = 125.65445899963379 | throughput = 32.597331066555604
step 14: loss = 5.731839179992676 | time = 125.93388557434082 | throughput = 32.52500295150557
step 15: loss = 5.718031883239746 | time = 125.67925453186035 | throughput = 32.590899868535125
step 16: loss = 5.691718101501465 | time = 125.85926055908203 | throughput = 32.5442878164473
step 17: loss = 5.65150260925293 | time = 125.67853927612305 | throughput = 32.591085348317506
step 18: loss = 5.621187210083008 | time = 126.04641914367676 | throughput = 32.49596480270562
step 19: loss = 5.611752510070801 | time = 125.66447257995605 | throughput = 32.59473354645923
step 20: loss = 5.6226325035095215 | time = 125.83470344543457 | throughput = 32.550638956097984
step 21: loss = 5.635449409484863 | time = 125.6864070892334 | throughput = 32.589045186819355
step 22: loss = 5.639663219451904 | time = 125.93579292297363 | throughput = 32.524510347151626
step 23: loss = 5.631251335144043 | time = 125.61678886413574 | throughput = 32.607106399051005
step 24: loss = 5.618274211883545 | time = 125.87618827819824 | throughput = 32.53991128923807
step 25: loss = 5.609709739685059 | time = 125.69999694824219 | throughput = 32.58552187305585
step 26: loss = 5.604081153869629 | time = 125.96678733825684 | throughput = 32.51650761721079
step 27: loss = 5.600447654724121 | time = 125.73957443237305 | throughput = 32.575265333055235
step 28: loss = 5.597555637359619 | time = 125.83112716674805 | throughput = 32.55156408614293
step 29: loss = 5.593827247619629 | time = 125.7634162902832 | throughput = 32.56908981023337
step 30: loss = 5.590880870819092 | time = 125.90241432189941 | throughput = 32.53313307819184
step 31: loss = 5.586056709289551 | time = 125.59962272644043 | throughput = 32.61156292579959
step 32: loss = 5.580429553985596 | time = 125.88930130004883 | throughput = 32.536521830695165
step 33: loss = 5.56891393661499 | time = 125.71978569030762 | throughput = 32.58039279584758
step 34: loss = 5.541508674621582 | time = 125.87285041809082 | throughput = 32.540774173262946
step 35: loss = 8.978800773620605 | time = 125.58102607727051 | throughput = 32.616392204660876
step 36: loss = 5.577319622039795 | time = 125.86569786071777 | throughput = 32.54262336456919
step 37: loss = 5.601972579956055 | time = 125.64373016357422 | throughput = 32.60011458325427
step 38: loss = 5.613892555236816 | time = 125.83112716674805 | throughput = 32.55156408614293
step 39: loss = 5.614576816558838 | time = 125.59342384338379 | throughput = 32.613172526515015
step 40: loss = 5.606247901916504 | time = 125.88763236999512 | throughput = 32.53695317711184
step 41: loss = 5.598392963409424 | time = 125.71406364440918 | throughput = 32.58187573655893
step 42: loss = 5.594179153442383 | time = 125.85949897766113 | throughput = 32.54422616704522
step 43: loss = 5.595061302185059 | time = 125.92911720275879 | throughput = 32.526234527674966
step 44: loss = 5.600915431976318 | time = 125.92530250549316 | throughput = 32.527219855765864
step 45: loss = 5.601868629455566 | time = 125.6716251373291 | throughput = 32.59287842839662
step 46: loss = 5.601120948791504 | time = 125.99658966064453 | throughput = 32.50881639758699
step 47: loss = 5.595525741577148 | time = 125.63967704772949 | throughput = 32.60116625772576
step 48: loss = 5.592442989349365 | time = 125.79131126403809 | throughput = 32.56186742025788
step 49: loss = 5.592747211456299 | time = 125.61750411987305 | throughput = 32.60692073686888
step 50: loss = 5.593912124633789 | time = 125.82159042358398 | throughput = 32.55403135670622

--------------------------------------------------------------

# ----- BASELINE CHANGE ----- #
changing the baseline metric from time to throughput
B = 4 ; T = 1024 ; dtype = float32
-------------------------------
cuda
loaded 338025 tokens
1 Epoch = 82 batches
step 1: loss = 10.953851699829102 | time = 669.363260269165 | throughput = 6.119248311227766
step 2: loss = 9.585126876831055 | time = 301.14126205444336 | throughput = 13.601590071238672
step 3: loss = 8.854998588562012 | time = 301.0528087615967 | throughput = 13.60558639811136
step 4: loss = 7.595379829406738 | time = 301.2728691101074 | throughput = 13.595648397078259
step 5: loss = 6.969884872436523 | time = 301.1786937713623 | throughput = 13.599899610127965
step 6: loss = 6.454421520233154 | time = 301.2223243713379 | throughput = 13.597929730303035
step 7: loss = 6.090400695800781 | time = 301.1608123779297 | throughput = 13.600707102821495
step 8: loss = 5.876863479614258 | time = 301.0578155517578 | throughput = 13.605360128230307
step 9: loss = 5.764042377471924 | time = 301.15699768066406 | throughput = 13.600879380339851
step 10: loss = 5.712063789367676 | time = 301.0239601135254 | throughput = 13.606890290245575
step 11: loss = 5.697848796844482 | time = 301.06401443481445 | throughput = 13.605079994995066
step 12: loss = 5.699055194854736 | time = 301.13720893859863 | throughput = 13.601773140014615
step 13: loss = 5.699883460998535 | time = 301.0737895965576 | throughput = 13.604638269869614
step 14: loss = 5.694967269897461 | time = 301.0716438293457 | throughput = 13.604735231464398
step 15: loss = 5.681886196136475 | time = 301.0084629058838 | throughput = 13.60759083136043
step 16: loss = 5.657900810241699 | time = 301.13983154296875 | throughput = 13.601654683185123
step 17: loss = 5.631113529205322 | time = 301.0067939758301 | throughput = 13.60766627855216
step 18: loss = 5.615684509277344 | time = 301.1789321899414 | throughput = 13.599888844206466
step 19: loss = 5.615324020385742 | time = 301.0594844818115 | throughput = 13.605284706609067
step 20: loss = 5.621796131134033 | time = 301.00250244140625 | throughput = 13.607860289458342
step 21: loss = 5.6294779777526855 | time = 301.12266540527344 | throughput = 13.60243007442597
step 22: loss = 5.628397464752197 | time = 301.0420799255371 | throughput = 13.606071287486278
step 23: loss = 5.621950149536133 | time = 301.0718822479248 | throughput = 13.604724457885613
step 24: loss = 5.617053985595703 | time = 301.10740661621094 | throughput = 13.603119385305352
step 25: loss = 5.610874176025391 | time = 300.9769916534424 | throughput = 13.609013690708649
step 26: loss = 5.607361316680908 | time = 301.0740280151367 | throughput = 13.604627496444397
step 27: loss = 5.604679107666016 | time = 301.17130279541016 | throughput = 13.600233362148948
step 28: loss = 5.604954719543457 | time = 301.03349685668945 | throughput = 13.606459223871518
step 29: loss = 5.6049299240112305 | time = 301.01943016052246 | throughput = 13.607095056341564
step 30: loss = 5.603055477142334 | time = 301.00536346435547 | throughput = 13.607730948239537
step 31: loss = 5.6010518074035645 | time = 301.33056640625 | throughput = 13.59304516913105
step 32: loss = 5.597942352294922 | time = 302.081823348999 | throughput = 13.55924019058849
step 33: loss = 5.595212459564209 | time = 300.950288772583 | throughput = 13.610221198675093
step 34: loss = 5.595129489898682 | time = 301.0296821594238 | throughput = 13.606631647143615
step 35: loss = 5.5932087898254395 | time = 300.9295463562012 | throughput = 13.611159321496764
step 36: loss = 5.592187881469727 | time = 301.0842800140381 | throughput = 13.604164255300953
step 37: loss = 5.590123176574707 | time = 301.0563850402832 | throughput = 13.605424775999785
step 38: loss = 5.586925983428955 | time = 300.9068965911865 | throughput = 13.612183856207338
step 39: loss = 5.585329055786133 | time = 301.04899406433105 | throughput = 13.605758799262844
step 40: loss = 5.581961631774902 | time = 300.98962783813477 | throughput = 13.608442355371574
step 41: loss = 5.581133842468262 | time = 301.0520935058594 | throughput = 13.605618722994462
step 42: loss = 5.578280448913574 | time = 300.97222328186035 | throughput = 13.609229301416622
step 43: loss = 5.577367782592773 | time = 300.98724365234375 | throughput = 13.608550150820005
step 44: loss = 5.574114799499512 | time = 300.8904457092285 | throughput = 13.612928088644766
step 45: loss = 5.570303440093994 | time = 300.93884468078613 | throughput = 13.610738767687955
step 46: loss = 5.566921234130859 | time = 301.0067939758301 | throughput = 13.60766627855216
step 47: loss = 5.563278675079346 | time = 301.01728439331055 | throughput = 13.607192052959151
step 48: loss = 5.560368061065674 | time = 301.0411262512207 | throughput = 13.606114390436682
step 49: loss = 5.55423641204834 | time = 300.9474277496338 | throughput = 13.61035058723802
step 50: loss = 5.548121929168701 | time = 301.01466178894043 | throughput = 13.607310606258618

--------------------------------------------------------------

# ----- BASELINE ----- #
B = 4 ; T = 1024 ; dtype = float32
------------------------
cuda
loaded 338025 tokens
1 Epoch = 82 batches
step 1: loss = 10.957832336425781 | time = 1589.1225337982178
step 2: loss = 9.744239807128906 | time = 1212.7771377563477
step 3: loss = 10.972354888916016 | time = 1230.9072017669678
step 4: loss = 8.774917602539062 | time = 1239.363670349121
step 5: loss = 7.651184558868408 | time = 1245.945692062378
step 6: loss = 6.87078857421875 | time = 1245.624303817749
step 7: loss = 6.329787731170654 | time = 1244.5142269134521
step 8: loss = 5.989801406860352 | time = 1257.246732711792
step 9: loss = 5.809932708740234 | time = 1256.6657066345215
step 10: loss = 5.739829063415527 | time = 1226.353645324707
step 11: loss = 5.726529121398926 | time = 1253.2141208648682
step 12: loss = 5.719888210296631 | time = 1255.2611827850342
step 13: loss = 5.71586799621582 | time = 1259.6900463104248
step 14: loss = 5.716298580169678 | time = 1260.7014179229736
step 15: loss = 5.709798336029053 | time = 1262.582778930664
step 16: loss = 5.691406726837158 | time = 1268.1748867034912
step 17: loss = 5.663188934326172 | time = 1268.4566974639893
step 18: loss = 5.636880397796631 | time = 1274.1649150848389
step 19: loss = 5.62070369720459 | time = 1275.9020328521729
step 20: loss = 5.6150031089782715 | time = 1272.2272872924805
step 21: loss = 5.619780540466309 | time = 1278.289794921875
step 22: loss = 5.62244987487793 | time = 1282.4633121490479
step 23: loss = 5.624851226806641 | time = 1289.5894050598145
step 24: loss = 5.618223190307617 | time = 1285.5684757232666
step 25: loss = 5.6100335121154785 | time = 1292.560338973999
step 26: loss = 5.600595474243164 | time = 1295.4905033111572
step 27: loss = 5.593338489532471 | time = 1298.9661693572998
step 28: loss = 5.587722301483154 | time = 1300.8184432983398
step 29: loss = 5.583390235900879 | time = 1293.1113243103027
step 30: loss = 5.575660705566406 | time = 1317.1510696411133
step 31: loss = 5.555104732513428 | time = 1307.7375888824463
step 32: loss = 5.494010925292969 | time = 1326.1082172393799
step 33: loss = 9.473379135131836 | time = 1327.1868228912354
step 34: loss = 5.924776077270508 | time = 1316.1354064941406
step 35: loss = 5.6139702796936035 | time = 1333.8656425476074
step 36: loss = 5.634500026702881 | time = 1317.8064823150635
step 37: loss = 5.63551139831543 | time = 1329.9860954284668
step 38: loss = 5.626481056213379 | time = 1320.6093311309814
step 39: loss = 5.609877109527588 | time = 1344.2010879516602
step 40: loss = 5.600749969482422 | time = 1335.1902961730957
step 41: loss = 5.597568511962891 | time = 1331.3891887664795
step 42: loss = 5.602120876312256 | time = 1336.2674713134766
step 43: loss = 5.612254619598389 | time = 1345.306634902954
step 44: loss = 5.614740371704102 | time = 1340.6474590301514
step 45: loss = 5.608066082000732 | time = 1354.4979095458984
step 46: loss = 5.599985599517822 | time = 1372.894048690796
step 47: loss = 5.594512939453125 | time = 1378.643274307251
step 48: loss = 5.5951409339904785 | time = 1382.4083805084229
step 49: loss = 5.598855972290039 | time = 1357.0613861083984
step 50: loss = 5.6001200675964355 | time = 1364.9451732635498

--------------------------------------------------------------

