{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff75118",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------TRAINING CODE------- #\n",
    "import time\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "print(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B=4, T=1024)\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "\n",
    "losses = []\n",
    "all_tokens_per_sec = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "for i in range(50):\n",
    "  # let's time\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad() # zero out all the gradients first so we go into each step without accumulating loss\n",
    "  logits, loss = model(x, y)\n",
    "  loss.backward() # backprop\n",
    "  optimizer.step() # update the params based on the backprop\n",
    "  torch.cuda.synchronize() # needs to make sure all the threads have completed on the gpu -- makes the cpu wait\n",
    "  t1 = time.time()\n",
    "  t = (t1-t0) * 1000 # miliseconds\n",
    "  losses.append(loss.item())\n",
    "  tokens_per_sec = train_loader.B * train_loader.T / t # a more objective metric which is throughput -- how many tokens are we getting through per second\n",
    "  all_tokens_per_sec.append(tokens_per_sec)\n",
    "  print(f\"step {i+1}: loss = {loss.item()} | throughput = {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fa84f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------TRAINING OUTPUT------- \n",
    "# cuda\n",
    "# loaded 338025 tokens\n",
    "# 1 Epoch = 2640 batches\n",
    "# step 1: loss = 10.967875480651855\n",
    "# step 2: loss = 9.800003051757812\n",
    "# step 3: loss = 9.50924015045166\n",
    "# step 4: loss = 7.666291236877441\n",
    "# step 5: loss = 6.684833526611328\n",
    "# step 6: loss = 5.924807548522949\n",
    "# step 7: loss = 5.279321670532227\n",
    "# step 8: loss = 4.746295928955078\n",
    "# step 9: loss = 4.422515869140625\n",
    "# step 10: loss = 4.2987494468688965\n",
    "# step 11: loss = 4.2793073654174805\n",
    "# step 12: loss = 4.296316623687744\n",
    "# step 13: loss = 4.316412925720215\n",
    "# step 14: loss = 4.339062690734863\n",
    "# step 15: loss = 4.346867084503174\n",
    "# step 16: loss = 4.324669361114502\n",
    "# step 17: loss = 4.277787685394287\n",
    "# step 18: loss = 4.224325180053711\n",
    "# step 19: loss = 4.16982364654541\n",
    "# step 20: loss = 4.141805171966553\n",
    "# step 21: loss = 4.15151309967041\n",
    "# step 22: loss = 4.181767463684082\n",
    "# step 23: loss = 4.207672595977783\n",
    "# step 24: loss = 4.204050064086914\n",
    "# step 25: loss = 4.189088821411133\n",
    "# step 26: loss = 4.175002574920654\n",
    "# step 27: loss = 4.1621198654174805\n",
    "# step 28: loss = 4.140615940093994\n",
    "# step 29: loss = 4.144059658050537\n",
    "# step 30: loss = 4.142584323883057\n",
    "# step 31: loss = 4.15804386138916\n",
    "# step 32: loss = 4.151178359985352\n",
    "# step 33: loss = 4.148890495300293\n",
    "# step 34: loss = 4.137920379638672\n",
    "# step 35: loss = 4.131983757019043\n",
    "# step 36: loss = 4.121450901031494\n",
    "# step 37: loss = 4.101718425750732\n",
    "# step 38: loss = 7.623427391052246\n",
    "# step 39: loss = 4.1219801902771\n",
    "# step 40: loss = 4.131641864776611\n",
    "# step 41: loss = 4.149787425994873\n",
    "# step 42: loss = 4.147574424743652\n",
    "# step 43: loss = 4.141959190368652\n",
    "# step 44: loss = 4.137613296508789\n",
    "# step 45: loss = 4.128176689147949\n",
    "# step 46: loss = 4.112405776977539\n",
    "# step 47: loss = 4.105103969573975\n",
    "# step 48: loss = 4.1128339767456055\n",
    "# step 49: loss = 4.114365100860596\n",
    "# step 50: loss = 4.110799789428711\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8878e22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------INFERENCE CODE------- \n",
    "Generating outputs from our model \n",
    "we are using the pretrained weights we got from hf_model but putting it through our gpt2 model on eval mode\n",
    "\n",
    "num_return_seq = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval() # probably does nothing we don't know\n",
    "model.to('cuda') # move all the tensors to the GPU\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2') # tokenizer for gpt2\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\") # 8 tokens\n",
    "x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_seq, 1).to('cuda') # [5, 8]\n",
    "\n",
    "# tokenized and ready to generate\n",
    "torch.manual_seed(420)\n",
    "torch.cuda.manual_seed(420)\n",
    "while x.size(1) < max_length:\n",
    "  with torch.no_grad():\n",
    "    logits = model(x) # goes through the entire network and gives us output logits\n",
    "    # logits = logits.logits\n",
    "    logits = logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1) # getting the top 50 probbailities -- everything else is set to 0 -- keeps the model on track\n",
    "    idx_next = torch.multinomial(topk_probs, 1)\n",
    "    xcol = torch.gather(topk_indices, -1, idx_next)\n",
    "    x = torch.cat((x, xcol), dim=-1)\n",
    "\n",
    "for i in range(num_return_seq):\n",
    "  tokens = x[i, :max_length].tolist()\n",
    "  decoded = enc.decode(tokens)\n",
    "  print(\">>\", decoded)\n",
    "\n",
    "\n",
    "## -------OUTPUT-------\n",
    "# >> Hello, I'm a language model, not a computer. You could call me a language model, with the same language as I'm writing. I\n",
    "# >> Hello, I'm a language model, not a programmer. I'm just doing what you call, writing things instead of just code. But it's\n",
    "# >> Hello, I'm a language model, that sorta, I'd like to know how it was constructed. So, I've built an\n",
    "# >> Hello, I'm a language model, a grammar. I'm very careful not to make mistakes or use an unfair definition of \"language\" to justify\n",
    "# >> Hello, I'm a language model, I'm an action model. Well, I think this is a good idea. In February, the"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
