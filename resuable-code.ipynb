{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff75118",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------TRAINING CODE------- #\n",
    "\n",
    "import time\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "  device = \"cuda\"\n",
    "print(device)\n",
    "\n",
    "train_loader = DataLoaderLite(B=8, T=1024)\n",
    "# optim #1\n",
    "torch.set_float32_matmul_precision('high') \n",
    "# high is for tf32 output for float32 matmuls\n",
    "# also tried 'medium' which is for bf16 -- but we don't use medium we use torch.autocast!!\n",
    "# we now expect all the matmuls (in Linear layers especially) to run tf32 -- expecting around 8x speedup\n",
    "\n",
    "model = GPT(GPTConfig())\n",
    "model.to(device)\n",
    "model = torch.compile(model) # optim #3 -- torch.compile\n",
    "\n",
    "losses = []\n",
    "all_tokens_per_sec = []\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "for i in range(50):\n",
    "  # let's time\n",
    "  t0 = time.time()\n",
    "  x, y = train_loader.next_batch()\n",
    "  x, y = x.to(device), y.to(device)\n",
    "  optimizer.zero_grad() # zero out all the gradients first so we go into each step without accumulating loss\n",
    "  # optim #2 -- torch.autocast : weights are in float32 and activations are in bfloat16 -- only select layers are changed\n",
    "  with torch.autocast(device_type=device, dtype=torch.float16):\n",
    "    logits, loss = model(x, y) \n",
    "  loss.backward() # backprop\n",
    "  optimizer.step() # update the params based on the backprop\n",
    "  torch.cuda.synchronize() # needs to make sure all the threads have completed on the gpu -- makes the cpu wait\n",
    "  t1 = time.time()\n",
    "  t = (t1-t0) * 1000 # miliseconds\n",
    "  losses.append(loss.item())\n",
    "  tokens_per_sec = (train_loader.B * train_loader.T) / t # a more objective metric which is throughput -- how many tokens are we getting through per second\n",
    "  all_tokens_per_sec.append(tokens_per_sec)\n",
    "  print(f\"step {i+1}: loss = {loss.item()} | time = {t} | throughput = {tokens_per_sec}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8878e22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------INFERENCE CODE------- \n",
    "Generating outputs from our model \n",
    "we are using the pretrained weights we got from hf_model but putting it through our gpt2 model on eval mode\n",
    "\n",
    "num_return_seq = 5\n",
    "max_length = 30\n",
    "\n",
    "\n",
    "model = GPT.from_pretrained(\"gpt2\")\n",
    "# model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "model.eval() # probably does nothing we don't know\n",
    "model.to('cuda') # move all the tensors to the GPU\n",
    "\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2') # tokenizer for gpt2\n",
    "tokens = enc.encode(\"Hello, I'm a language model,\") # 8 tokens\n",
    "x = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_return_seq, 1).to('cuda') # [5, 8]\n",
    "\n",
    "# tokenized and ready to generate\n",
    "torch.manual_seed(420)\n",
    "torch.cuda.manual_seed(420)\n",
    "while x.size(1) < max_length:\n",
    "  with torch.no_grad():\n",
    "    logits = model(x) # goes through the entire network and gives us output logits\n",
    "    # logits = logits.logits\n",
    "    logits = logits[:, -1, :]\n",
    "    probs = F.softmax(logits, dim=-1)\n",
    "    topk_probs, topk_indices = torch.topk(probs, k=50, dim=-1) # getting the top 50 probbailities -- everything else is set to 0 -- keeps the model on track\n",
    "    idx_next = torch.multinomial(topk_probs, 1)\n",
    "    xcol = torch.gather(topk_indices, -1, idx_next)\n",
    "    x = torch.cat((x, xcol), dim=-1)\n",
    "\n",
    "for i in range(num_return_seq):\n",
    "  tokens = x[i, :max_length].tolist()\n",
    "  decoded = enc.decode(tokens)\n",
    "  print(\">>\", decoded)\n",
    "\n",
    "\n",
    "## -------OUTPUT-------\n",
    "# >> Hello, I'm a language model, not a computer. You could call me a language model, with the same language as I'm writing. I\n",
    "# >> Hello, I'm a language model, not a programmer. I'm just doing what you call, writing things instead of just code. But it's\n",
    "# >> Hello, I'm a language model, that sorta, I'd like to know how it was constructed. So, I've built an\n",
    "# >> Hello, I'm a language model, a grammar. I'm very careful not to make mistakes or use an unfair definition of \"language\" to justify\n",
    "# >> Hello, I'm a language model, I'm an action model. Well, I think this is a good idea. In February, the"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
